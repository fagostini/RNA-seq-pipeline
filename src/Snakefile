import os
import platform
import pandas as pd
import numpy as np
import string
import re
from itertools import chain

# # Import config file & parameters
# configfile: 'config.yaml'

# Import paths from config file
PATH = "{}".format(config['global']['snakemake'])
RESULTS_PATH = "{}".format(config['global']['results'])
GENOME_PATH = "{}/{}".format(config['genome']['path'], config['genome']['name'])
ANNOTATION_PATH = "{}/{}".format(config['annotation']['path'], config['annotation']['name'])
RIBO_INDEX_PATH = "{}/{}".format(config['alignment']['path'], config['alignment']['ribo'])
STAR_INDEX_PATH = "{}/{}".format(config['alignment']['path'], config['alignment']['star'])

def platform_info():
	import platform
	if platform.system() in "Darwin":
		return(".".join(("macOS", platform.machine())))
	else:
		return(".".join((platform.system().lower(), platform.machine())))

rule map:
	input:
		RESULTS_PATH+"/sampleMetrics.tsv",
		RESULTS_PATH+"/sampleTable.tsv"

rule all:
	input:
		RESULTS_PATH+"/sampleMetrics.tsv",
		RESULTS_PATH+"/sampleTableLong.tsv"

def rename_out(inlist, suffix, outdir, outlist):
	for i in range(len(inlist)):
		base_out = "/".join((outdir, os.path.basename(inlist[i])))
		temp_out = "".join((base_out.partition(".")[0], suffix[0] if len(suffix)==1 else suffix[i]))
		print(" ".join(("mv", temp_out, outlist[i])))
		os.system(" ".join(("mv", temp_out, outlist[i])))

rule fastqc_pre_SE:
	input:
		fastq=lambda wildcards: config["samples"][wildcards.sample]
	output:
		html=expand(RESULTS_PATH+"/fastqc-pre/{{sample}}_{ext}_fastqc.html", ext=["SE"]),
		gzip=temp(expand(RESULTS_PATH+"/fastqc-pre/{{sample}}_{ext}_fastqc.zip", ext=["SE"]))
	params:
		outdir=RESULTS_PATH+"/fastqc-pre",
		cluster='-N 1 -c 1 --mem=8G -t 160:00:00 -o logs/fastqc_pre.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/fastqc-pre/{sample}.txt"
	threads:
		1
	run:
		shell("fastqc -t {threads} --outdir {params.outdir} {input.fastq} 2>&1 > {log}")
		rename_out(input["fastq"], ["_fastqc.html"], params["outdir"], output['html'])
		rename_out(input["fastq"], ["_fastqc.zip"], params["outdir"], output['gzip'])

rule fastqc_pre_PE:
	input:
		fastq=lambda wildcards: config["samples"][wildcards.sample]
	output:
		html=expand(RESULTS_PATH+"/fastqc-pre/{{sample}}_{ext}_fastqc.html", ext=["F", "R"]),
		gzip=temp(expand(RESULTS_PATH+"/fastqc-pre/{{sample}}_{ext}_fastqc.zip", ext=["F", "R"]))
	params:
		outdir=RESULTS_PATH+"/fastqc-pre",
		cluster='-N 1 -c 2 --mem=8G -t 160:00:00 -o logs/fastqc_pre.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/fastqc-pre/{sample}.txt"
	threads:
		2
	run:
		shell("fastqc -t {threads} --outdir {params.outdir} {input.fastq} 2>&1 > {log}")
		rename_out(input["fastq"], ["_fastqc.html"], params["outdir"], output['html'])
		rename_out(input["fastq"], ["_fastqc.zip"], params["outdir"], output['gzip'])

rule trimgalore_SE:
	input:
		# html=expand(RESULTS_PATH+"/fastqc-pre/{{sample}}_{ext}_fastqc.html", ext=["SE"]),
		fastq=lambda wildcards: config["samples"][wildcards.sample]
	output:
		fastq=temp(expand(RESULTS_PATH+"/trim_galore/{{sample}}_{ext}_trimmed.fq.gz", ext=["SE"]))
	params:
		basic="-q 20 --gzip --length 16 --no_report_file",
		outdir=RESULTS_PATH+"/trim_galore",
		cluster='-N 1 -c 1 --mem=16G -t 160:00:00 -o logs/trimgalore.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/trim_galore/{sample}_SE.fq.gz_trimming_report.txt",
	run:
		shell("trim_galore {params.basic} -o {params.outdir} {input.fastq} &> {log}")
		rename_out(input["fastq"], ["_trimmed.fq.gz"], params["outdir"], output['fastq'])

rule trimgalore_PE:
	input:
		# html=expand(RESULTS_PATH+"/fastqc-pre/{{sample}}_{ext}_fastqc.html", ext=["F", "R"]),
		fastq=lambda wildcards: config["samples"][wildcards.sample]
	output:
		fastq=temp(expand(RESULTS_PATH+"/trim_galore/{{sample}}_{ext}_trimmed.fq.gz", ext=["F", "R"]))
	params:
		basic="-q 20 --gzip --length 16 --no_report_file --paired --trim1",
		outdir=RESULTS_PATH+"/trim_galore",
		cluster='-N 1 -c 1 --mem=16G -t 160:00:00 -o logs/trimgalore.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/trim_galore/{sample}_PE.fq.gz_trimming_report.txt"
	run:
		shell("trim_galore {params.basic} -o {params.outdir} {input.fastq} &> {log}")
		rename_out(input["fastq"], ["_val_1.fq.gz", "_val_2.fq.gz"], params["outdir"], output['fastq'])

rule index_rRNA_tRNA:
	input:
		RIBO_INDEX_PATH+".fa.gz"
	output:
		expand(RIBO_INDEX_PATH+".{count}.bt2", count=['1', '2', '3', '4']),
		expand(RIBO_INDEX_PATH+".rev.{count}.bt2", count=['1', '2'])
	params:
		basename=RIBO_INDEX_PATH,
		cluster='-N 1 -c 8 --mem=30G -t 160:00:00 -o logs/index_rRNA_tRNA.%A.log'
	log:
		"logs/index_rRNA_tRNA/index_rRNA_tRNA.log"
	threads:
		12
	shell:
		"bowtie2-build --threads {threads} {input} {params.basename} &> {log}"

rule remove_rRNA_tRNA_SE:
	input:
		fastq=expand(RESULTS_PATH+"/trim_galore/{{sample}}_{ext}_trimmed.fq.gz", ext=["SE"]),
		index1=expand(RIBO_INDEX_PATH+".{count}.bt2", count=['1', '2', '3', '4']),
		index2=expand(RIBO_INDEX_PATH+".rev.{count}.bt2", count=['1', '2'])
	output:
		fastq=temp(expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{ext}_rRNA_tRNA_removed.fq.gz", ext=["SE"])),
		tmp=temp(expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{tmp}.gz", tmp=["temp"]))
	params:
		basic="--sensitive-local",
		basename=RIBO_INDEX_PATH,
		cluster='-N 1 -c 8 --mem=32G -t 160:00:00 -o logs/remove_rRNA_tRNA.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/rRNA_tRNA_removed/{sample}_SE_rRNA_tRNA_removed.log"
	threads:
		12
	run:
		shell("bowtie2 {params.basic} --threads {threads} --un-gz {output.fastq} -x {params.basename} -U {input.fastq} 2> {log} | gzip -c > {output.tmp}")

rule remove_rRNA_tRNA_PE:
	input:
		fastq=expand(RESULTS_PATH+"/trim_galore/{{sample}}_{ext}_trimmed.fq.gz", ext=["F", "R"]),
		index1=expand(RIBO_INDEX_PATH+".{count}.bt2", count=['1', '2', '3', '4']),
		index2=expand(RIBO_INDEX_PATH+".rev.{count}.bt2", count=['1', '2'])
	output:
		fastq=temp(expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{ext}_rRNA_tRNA_removed.fq.gz", ext=["F", "R"])),
		tmp=temp(expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{tmp}.gz", tmp=["temp"]))
	params:
		basic="--sensitive-local",
		basename=RIBO_INDEX_PATH,
		outname=RESULTS_PATH+"/rRNA_tRNA_removed/{sample}.fq.gz",
		outdir=RESULTS_PATH+"/rRNA_tRNA_removed",
		cluster='-N 1 -c 8 --mem=48G -t 160:00:00 -o logs/remove_rRNA_tRNA.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/rRNA_tRNA_removed/{sample}_PE_rRNA_tRNA_removed.log"
	threads:
		12
	run:
		fq1 = input["fastq"][0] 
		fq2 = input["fastq"][1] 
		shell("bowtie2 {params.basic} -p {threads} --un-conc-gz {params.outname} -x {params.basename} -1 {fq1} -2 {fq2} 2> {log} | gzip -c > {output.tmp}")
		inputList = [params["outname"].replace(".fq.gz", s) for s in [".fq.1.gz", ".fq.2.gz"]]
		# shell("python {}/scripts/pair_fastq_fast.py -l {} -r {}".format(PATH, *inputList))
		rename_out(inputList, [".fq.1.gz", ".fq.2.gz"], params["outdir"], output['fastq'])

rule fastqc_post_SE:
	input:
		fastq=expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{ext}_rRNA_tRNA_removed.fq.gz", ext=["SE"])
	output:
		html=expand(RESULTS_PATH+"/fastqc-post/{{sample}}_{ext}_fastqc.html", ext=["SE"]),
		gzip=temp(expand(RESULTS_PATH+"/fastqc-post/{{sample}}_{ext}_fastqc.zip", ext=["SE"]))
	params:
		outdir=RESULTS_PATH+"/fastqc-post",
		cluster='-N 1 -c 1 --mem=8G -t 160:00:00 -o logs/fastqc_post.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/fastqc-post/{sample}.txt"
	threads:
		1
	run:
		shell("fastqc -t {threads} --outdir {params.outdir} {input.fastq} 2>&1 > {log}")
		rename_out(input["fastq"], ["_fastqc.html"], params["outdir"], output['html'])
		rename_out(input["fastq"], ["_fastqc.zip"], params["outdir"], output['gzip'])

rule fastqc_post_PE:
	input:
		fastq=expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{ext}_rRNA_tRNA_removed.fq.gz", ext=["F", "R"])
	output:
		html=expand(RESULTS_PATH+"/fastqc-post/{{sample}}_{ext}_fastqc.html", ext=["F", "R"]),
		gzip=temp(expand(RESULTS_PATH+"/fastqc-post/{{sample}}_{ext}_fastqc.zip", ext=["F", "R"]))
	params:
		outdir=RESULTS_PATH+"/fastqc-post",
		cluster='-N 1 -c 2 --mem=8G -t 160:00:00 -o logs/fastqc_post.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/fastqc-post/{sample}.txt"
	threads:
		2
	run:
		shell("fastqc -t {threads} --outdir {params.outdir} {input.fastq} 2>&1 > {log}")
		rename_out(input["fastq"], ["_fastqc.html"], params["outdir"], output['html'])
		rename_out(input["fastq"], ["_fastqc.zip"], params["outdir"], output['gzip'])

rule gunzip_annotation:
	input:
		ANNOTATION_PATH
	output:
		os.path.splitext(ANNOTATION_PATH)[0]
	params:
		cluster='-N 1 -c 1 --mem=8G -t 160:00:00 -o logs/gunzip_annotation.%A.log'
	shell:
		"gunzip {input}"

rule gunzip_genome:
	input:
		GENOME_PATH
	output:
		os.path.splitext(GENOME_PATH)[0]
	params:
		cluster='-N 1 -c 1 --mem=8G -t 160:00:00 -o logs/gunzip_genome.%A.log'
	shell:
		"gunzip {input}"

rule index_star:
	input:
		genome=os.path.splitext(GENOME_PATH)[0] if os.path.splitext(GENOME_PATH)[1] == ".gz" else GENOME_PATH,
		annotation=os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH
	output:
		chrmlen=STAR_INDEX_PATH+"/chrNameLength.txt",
		SA=STAR_INDEX_PATH+"/SAindex"
	params:
		outdir=STAR_INDEX_PATH,
		overhang="--sjdbOverhang 99",
		cluster='-N 1 -c 8 --mem=64G -t 160:00:00 -o logs/index_star.%A.log'
	threads:
		24
	run:
		shell("STAR --runMode genomeGenerate --runThreadN {threads} --genomeDir {params.outdir} --genomeFastaFiles {input.genome} --sjdbGTFfile {input.annotation} {params.overhang}")

def define_strand(counts, outfile):
	df = pd.read_table(counts, sep="\t", index_col=0, names=["Unstranded", "FirstStrand", "SecondStrand"])
	df = df.filter(regex="^(?!N_)", axis=0, ).sum()

	with open(outfile, "w") as of:
		if df['FirstStrand']/(df['FirstStrand']+df['SecondStrand']) > 0.55 :
			of.write("FirstStrand")
		elif df['FirstStrand']/(df['FirstStrand']+df['SecondStrand']) < 0.45 :
			of.write("SecondStrand")
		else:
			of.write("Unstranded")

	return(0)

rule map_star_SE:
	input:
		# html=expand(RESULTS_PATH+"/fastqc-post/{{sample}}_{ext}_fastqc.html", ext=["SE"]),
		fastq=expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{ext}_rRNA_tRNA_removed.fq.gz", ext=["SE"]),
		chrmlen=STAR_INDEX_PATH+"/chrNameLength.txt"
	output:
		tmp=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.out.bam", ext=["SE"])),
		# bam=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"])),
		cnt=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.ReadsPerGene.out.tab", ext=["SE"]),
		jct=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.SJ.out.tab", ext=["SE"]),
		sge=temp(directory(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}._STARgenome", ext=["SE"]))),
		spa=temp(directory(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}._STARpass1", ext=["SE"]))),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["SE"]),
		# wig=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Signal.Unique.str{num}.out.wig", ext=["SE"], num=["1", "2"])),
		# mul=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Signal.UniqueMultiple.str{num}.out.wig", ext=["SE"], num=["1", "2"])),
		log=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Log.final.out", ext=["SE"])
	params:
		genome=STAR_INDEX_PATH,
		outdir=RESULTS_PATH+"/mapped",
		alignemnt="--readFilesCommand zcat --genomeLoad NoSharedMemory --twopassMode Basic --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 --sjdbScore 1 --limitBAMsortRAM 60000000000",
		rna="--alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --quantMode GeneCounts",
		dna="--alignIntronMax 1 --alignMatesGapMax 300 --quantMode GeneCounts",
		# unstranded="--outSAMstrandField intronMotif",
		output="--outFilterMultimapNmax 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --outFilterType BySJout --outSAMattributes All --outSAMtype BAM SortedByCoordinate",
		wiggle="--outWigType wiggle --outWigStrand Stranded --outWigNorm RPM",
		cluster='-N 1 -c 8 --mem=64G -t 160:00:00 -o logs/map_star.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/star_mapped/{{sample}}_{ext}.txt", ext=["SE"])
	threads:
		12
	run:
		prefix = "/".join((params["outdir"], os.path.basename(output["tmp"][0]).partition(".")[0]+"."))
		if config['seq_type'] == "RNAseq":
			shell("STAR --runMode alignReads --runThreadN {threads} --genomeDir {params.genome} --readFilesIn {input.fastq} --outFileNamePrefix {prefix} {params.alignemnt} {params.rna} {params.output} 2>&1 > {log}")
			define_strand(output["cnt"][0], output["std"][0])
			# shell("sambamba view -t {threads} -h {output.tmp} | awk -f scripts/tagXSstrandedData.awk | sambamba view -t {threads} -S -f bam /dev/stdin > {output.bam}")
	
		elif config['seq_type'] == "NETseq":
			shell("STAR --runMode alignReads --runThreadN {threads} --genomeDir {params.genome} --readFilesIn {input.fastq} --outFileNamePrefix {prefix} {params.alignemnt} {params.dna} {params.output} 2>&1 > {log}")
			define_strand(output["cnt"][0], output["std"][0])
			# shell("sambamba view -t {threads} -h {output.tmp} | awk -f scripts/tagXSstrandedData.awk | sambamba view -t {threads} -S -f bam /dev/stdin > {output.bam}")

rule filter_unique_SE:
	input:
		expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.out.bam", ext=["SE"])
	output:
		temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"]))
	params:
		cluster='-N 1 -c 8 --mem=32G -t 160:00:00 -o logs/filter_unique.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/filter_unique/{{sample}}_{ext}.txt", ext=["SE"])
	threads:
		6
	run:
		scriptfile=PATH+"/scripts/tagXSstrandedData.awk"
		shell("sambamba view -t {threads} -h {input} 2> {log} | awk -f {scriptfile} 2>> {log} | sambamba view -t {threads} -S -f bam /dev/stdin 2>> {log} > {output}")

rule remove_duplicates_SE:
	input:
		exe=PATH+"/executables/jar/picard.jar",
		bamfile=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"]),
	output:
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"]),
		metrics=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.txt", ext=["SE"]),
		index=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam.bai", ext=["SE"])
	params:
		basic="REMOVE_DUPLICATES={} ASSUME_SORT_ORDER=coordinate".format(config['global']['remove_duplicates']),
		cluster='-N 1 -c 1 --mem=32G -t 160:00:00 -o logs/duplicates.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/remove_duplicates/{{sample}}_{ext}.txt", ext=["SE"])
	shell:
		"""
		java -Xmx12G -jar {input.exe} MarkDuplicates I={input.bamfile} O={output.bamfile} M={output.metrics} {params.basic} > {log}
		java -Xmx12G -jar {input.exe} BuildBamIndex I={output.bamfile} O={output.index}
		"""

rule map_star_PE:
	input:
		# html=expand(RESULTS_PATH+"/fastqc-post/{{sample}}_{ext}_fastqc.html", ext=["F", "R"]),
		fastq=expand(RESULTS_PATH+"/rRNA_tRNA_removed/{{sample}}_{ext}_rRNA_tRNA_removed.fq.gz", ext=["F", "R"]),
		index=STAR_INDEX_PATH+"/SAindex",
		chrmlen=STAR_INDEX_PATH+"/chrNameLength.txt"
	output:
		tmp=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.out.bam", ext=["PE"])),
		# bam=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"])),
		cnt=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.ReadsPerGene.out.tab", ext=["PE"]),
		jct=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.SJ.out.tab", ext=["PE"]),
		sge=temp(directory(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}._STARgenome", ext=["PE"]))),
		spa=temp(directory(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}._STARpass1", ext=["PE"]))),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["PE"]),
		# wig=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Signal.Unique.str{num}.out.wig", ext=["PE"], num=["1", "2"])),
		# mul=temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Signal.UniqueMultiple.str{num}.out.wig", ext=["PE"], num=["1", "2"])),
		log=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Log.final.out", ext=["PE"])
	params:
		genome=STAR_INDEX_PATH,
		outdir=RESULTS_PATH+"/mapped",
		alignemnt="--readFilesCommand zcat --genomeLoad NoSharedMemory --twopassMode Basic --alignSJoverhangMin 8 --alignSJDBoverhangMin 1 --sjdbScore 1 --limitBAMsortRAM 60000000000",
		rna="--alignIntronMin 20 --alignIntronMax 1000000 --alignMatesGapMax 1000000 --quantMode GeneCounts",
		dna="--alignIntronMax 1 --alignMatesGapMax 300 --quantMode GeneCounts",
		# unstranded="--outSAMstrandField intronMotif",
		output="--outFilterMultimapNmax 1 --outFilterMismatchNmax 999 --outFilterMismatchNoverReadLmax 0.04 --outFilterType BySJout --outSAMattributes All --outSAMtype BAM SortedByCoordinate",
		wiggle="--outWigType wiggle --outWigStrand Stranded --outWigNorm RPM",
		cluster='-N 1 -c 8 --mem=64G -t 160:00:00 -o logs/map_star.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/star_mapped/{{sample}}_{ext}.txt", ext=["PE"])
	threads:
		12
	run:
		prefix = "/".join((params["outdir"], os.path.basename(output["tmp"][0]).partition(".")[0]+"."))
		if config['seq_type'] == "RNAseq":
			shell("STAR --runMode alignReads --runThreadN {threads} --genomeDir {params.genome} --readFilesIn {input.fastq} --outFileNamePrefix {prefix} {params.alignemnt} {params.rna} {params.output} 2>&1 > {log}")
			define_strand(output["cnt"][0], output["std"][0])
			# with open(output["std"][0], "r") as infile:
			# 	first_line = infile.readline()
			# 	if first_line == "FirstStrand":
			# 		strand = 1
			# 	elif first_line == "SecondStrand":
			# 		strand = 2
			# 	else:
			# 		strand = 0
			# shell("sambamba view -t {threads} -h {output.tmp} | awk -v strType={strand} -f scripts/tagXSstrandedData.awk | sambamba view -t {threads} -S -f bam /dev/stdin > {output.bam}")
	
		elif config['seq_type'] == "NETseq":
			shell("STAR --runMode alignReads --runThreadN {threads} --genomeDir {params.genome} --readFilesIn {input.fastq} --outFileNamePrefix {prefix} {params.alignemnt} {params.dna} {params.output} 2>&1 > {log}")
			define_strand(output["cnt"][0], output["std"][0])
			# with open(output["std"][0], "r") as infile:
			# 	first_line = infile.readline()
			# 	if first_line == "FirstStrand":
			# 		strand = 1
			# 	elif first_line == "SecondStrand":
			# 		strand = 2
			# 	else:
			# 		strand = 0
			# shell("sambamba view -t {threads} -h {output.tmp} | awk -v strType={strand} -f scripts/tagXSstrandedData.awk | sambamba view -t {threads} -S -f bam /dev/stdin > {output.bam}")

rule filter_unique_PE:
	input:
		bam=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.out.bam", ext=["PE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["PE"])
	output:
		temp(expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"]))
	params:
		cluster='-N 1 -c 8 --mem=32G -t 160:00:00 -o logs/filter_unique.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/filter_unique/{{sample}}_{ext}.txt", ext=["PE"])
	threads:
		6
	run:
		with open(input["std"][0], "r") as infile:
			first_line = infile.readline()
			if first_line == "FirstStrand":
				strand = 1
			elif first_line == "SecondStrand":
				strand = 2
			else:
				strand = 0
		scriptfile=PATH+"/scripts/tagXSstrandedData.awk"
		shell("sambamba view -t {threads} -h {input.bam} 2> {log} | awk -v strType={strand} -f {scriptfile} 2>> {log} | sambamba view -t {threads} -S -f bam /dev/stdin 2>> {log} > {output}")

rule remove_duplicates_PE:
	input:
		exe=PATH+"/executables/jar/picard.jar",
		bamfile=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"]),
	output:
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"]),
		metrics=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.txt", ext=["PE"]),
		index=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam.bai", ext=["PE"])
	params:
		basic="REMOVE_DUPLICATES={} ASSUME_SORT_ORDER=coordinate".format(config['global']['remove_duplicates']),
		cluster='-N 1 -c 1 --mem=32G -t 160:00:00 -o logs/duplicates.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/remove_duplicates/{{sample}}_{ext}.txt", ext=["PE"])
	shell:
		"""
		java -Xmx12G -jar {input.exe} MarkDuplicates I={input.bamfile} O={output.bamfile} M={output.metrics} {params.basic} > {log}
		java -Xmx12G -jar {input.exe} BuildBamIndex I={output.bamfile} O={output.index}
		"""

def create_bigWig(exe_path, stdfile, bamfile, chrmlen, bedlist, biglist, bedg, bigw, btparam):
	shell(" ".join(("genomeCoverageBed -bg -split", btparam, "-strand + -ibam", bamfile, "| LC_COLLATE=C sort -k1,1 -k2,2n >", bedlist[0])))
	shell(" ".join(("genomeCoverageBed -bg -split", btparam, "-strand - -ibam", bamfile, "| LC_COLLATE=C sort -k1,1 -k2,2n >", bedlist[1])))

	with open(stdfile, "r") as infile:
		first_line = infile.readline()
	if first_line == "FirstStrand":
		shell(" ".join((PATH+"/executables/"+exe_path+"/bedGraphToBigWig", bedlist[0], chrmlen, biglist[0])))
		shell(" ".join((PATH+"/executables/"+exe_path+"/bedGraphToBigWig", bedlist[1], chrmlen, biglist[1])))
	elif first_line == "SecondStrand":
		shell(" ".join((PATH+"/executables/"+exe_path+"/bedGraphToBigWig", bedlist[1], chrmlen, biglist[0])))
		shell(" ".join((PATH+"/executables/"+exe_path+"/bedGraphToBigWig", bedlist[0], chrmlen, biglist[1])))
	else:
		shell(" ".join((PATH+"/executables/"+exe_path+"/bedGraphToBigWig", bedlist[0], chrmlen, biglist[0])))
		shell(" ".join((PATH+"/executables/"+exe_path+"/bedGraphToBigWig", bedlist[1], chrmlen, biglist[1])))

	shell(" ".join((PATH+"/executables/"+exe_path+"/bigWigMerge", *biglist, bedg)))
	shell(" ".join(("LC_COLLATE=C sort -k1,1 -k2,2n", bedg, ">", os.path.splitext(bedg)[0]+".sorted")))
	shell(" ".join(("mv", os.path.splitext(bedg)[0]+".sorted", bedg)))
	shell(" ".join((PATH+"/executables/"+exe_path+"/bedGraphToBigWig", bedg, chrmlen, bigw)))

	return(0)

rule bamToBigWig_SE:
	input:
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["SE"]),
		chrmlen=STAR_INDEX_PATH+"/chrNameLength.txt"
	output:
		bgl=temp(expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}.Signal.Unique.str{num}.out.bedGraph", ext=["SE"], num=["1", "2"])),
		bwi=expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}_{typ}.bigwig", ext=["SE"], typ=["plus", "minus"]),
		bgu=temp(expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}.bedGraph", ext=["SE"])),
		bwu=expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}.bigwig", ext=["SE"])
	params:
		bedtools="",
		cluster='-N 1 -c 1 --mem=32G -t 160:00:00 -o logs/bamToBigWig.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/bamToBigWig/{{sample}}_{ext}.txt", ext=["SE"])
	threads:
		1
	run:
		create_bigWig(platform_info(), input["std"][0], input["bamfile"][0], input["chrmlen"], output["bgl"], output["bwi"], output["bgu"][0], output["bwu"][0], params["bedtools"])

rule bamToBigWig_PE:
	input:
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["PE"]),
		chrmlen=STAR_INDEX_PATH+"/chrNameLength.txt"
	output:
		bgl=temp(expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}.Signal.Unique.str{num}.out.bedGraph", ext=["PE"], num=["1", "2"])),
		bwi=expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}_{typ}.bigwig", ext=["PE"], typ=["plus", "minus"]),
		bgu=temp(expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}.bedGraph", ext=["PE"])),
		bwu=expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}.bigwig", ext=["PE"])
	params:
		bedtools="-pc -du",
		cluster='-N 1 -c 1 --mem=32G -t 160:00:00 -o logs/bamToBigWig.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/bamToBigWig/{{sample}}_{ext}.txt", ext=["PE"])
	threads:
		1
	run:
		create_bigWig(platform_info(), input["std"][0], input["bamfile"][0], input["chrmlen"], output["bgl"], output["bwi"], output["bgu"][0], output["bwu"][0], params["bedtools"])

rule bamToBigWigCPM_SE:
	input:
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["SE"])
	output:
		bwi=expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}_{typ}_CPM.bigwig", ext=["SE"], typ=["plus", "minus", "unstranded"])
	params:
		deeptools="--normalizeUsing CPM --binSize 20 --smoothLength 60",
		cluster='-N 1 -c 8 --mem=16G -t 160:00:00 -o logs/bamToBigWigCPM.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/bamToBigWigCPM/{{sample}}_{ext}.txt", ext=["SE"])
	threads:
		6
	run:
		with open(input.std[0], "r") as infile:
			first_line = infile.readline()
		if first_line == "SecondStrand":
			shell("bamCoverage -b {} -o {} --filterRNAstrand forward --numberOfProcessors {} {} > {}".format(input.bamfile, output.bwi[0], threads, params.deeptools, log[0]))
			shell("bamCoverage -b {} -o {} --filterRNAstrand reverse --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[1], threads, params.deeptools, log[0]))
		elif first_line == "FirstStrand":
			shell("bamCoverage -b {} -o {} --filterRNAstrand reverse --numberOfProcessors {} {} > {}".format(input.bamfile, output.bwi[0], threads, params.deeptools, log[0]))
			shell("bamCoverage -b {} -o {} --filterRNAstrand forward --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[1], threads, params.deeptools, log[0]))
		else:
			shell("bamCoverage -b {} -o {} --filterRNAstrand reverse --numberOfProcessors {} {} > {}".format(input.bamfile, output.bwi[0], threads, params.deeptools, log[0]))
			shell("bamCoverage -b {} -o {} --filterRNAstrand forward --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[1], threads, params.deeptools, log[0]))
		shell("bamCoverage -b {} -o {} --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[2], threads, params.deeptools, log[0]))

rule bamToBigWigCPM_PE:
	input:
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["PE"])
	output:
		bwi=expand(RESULTS_PATH+"/coverage/{{sample}}_{ext}_{typ}_CPM.bigwig", ext=["PE"], typ=["plus", "minus", "unstranded"])
	params:
		deeptools="--normalizeUsing CPM --binSize 20 --smoothLength 60",
		cluster='-N 1 -c 8 --mem=16G -t 160:00:00 -o logs/bamToBigWigCPM.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/bamToBigWigCPM/{{sample}}_{ext}.txt", ext=["PE"])
	threads:
		6
	run:
		with open(input.std[0], "r") as infile:
			first_line = infile.readline()
		if first_line == "SecondStrand":
			shell("bamCoverage -b {} -o {} --filterRNAstrand forward --numberOfProcessors {} {} > {}".format(input.bamfile, output.bwi[0], threads, params.deeptools, log[0]))
			shell("bamCoverage -b {} -o {} --filterRNAstrand reverse --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[1], threads, params.deeptools, log[0]))
		elif first_line == "FirstStrand":
			shell("bamCoverage -b {} -o {} --filterRNAstrand reverse --numberOfProcessors {} {} > {}".format(input.bamfile, output.bwi[0], threads, params.deeptools, log[0]))
			shell("bamCoverage -b {} -o {} --filterRNAstrand forward --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[1], threads, params.deeptools, log[0]))
		else:
			shell("bamCoverage -b {} -o {} --filterRNAstrand reverse --numberOfProcessors {} {} > {}".format(input.bamfile, output.bwi[0], threads, params.deeptools, log[0]))
			shell("bamCoverage -b {} -o {} --filterRNAstrand forward --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[1], threads, params.deeptools, log[0]))
		shell("bamCoverage -b {} -o {} --numberOfProcessors {} {} >> {}".format(input.bamfile, output.bwi[2], threads, params.deeptools, log[0]))

rule stringtie_SE:
	input:
		bam=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["SE"]),
		annotation=os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH
	output:
		gtf=expand(RESULTS_PATH+"/stringtie/{{sample}}_{ext}.StringTie.gtf", ext=["SE"]),
		exp=expand(RESULTS_PATH+"/stringtie/{{sample}}_{ext}.geneAbundance.gtf", ext=["SE"])
	params:
		basic="-f 0.2 -m 200 -g 100 -a 10 -j 3 -t",
		cluster='-N 1 -c 8 --mem=32G -t 160:00:00 -o logs/stringtie.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/stringtie/{{sample}}_{ext}.txt", ext=["SE"])
	threads:
		6
	run:
		with open(input["std"][0], "r") as infile:
			first_line = infile.readline()
		if first_line == "FirstStrand":
			strand = "--rf"
		elif first_line == "SecondStrand":
			strand = "--fr"
		else:
			strand = ""
		shell(PATH+"/executables/"+platform_info()+"/stringtie {input.bam} -G {input.annotation} {strand} -o {output.gtf} -p {threads} -A {output.exp} {params.basic} > {log}")

rule stringtie_PE:
	input:
		bam=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["PE"]),
		annotation=os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH
	output:
		gtf=expand(RESULTS_PATH+"/stringtie/{{sample}}_{ext}.StringTie.gtf", ext=["PE"]),
		exp=expand(RESULTS_PATH+"/stringtie/{{sample}}_{ext}.geneAbundance.gtf", ext=["PE"])
	params:
		basic="-f 0.2 -m 200 -g 100 -a 10 -j 3 -t",
		cluster='-N 1 -c 8 --mem=32G -t 160:00:00 -o logs/stringtie.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/stringtie/{{sample}}_{ext}.txt", ext=["PE"])
	threads:
		6
	run:
		with open(input["std"][0], "r") as infile:
			first_line = infile.readline()
		if first_line == "FirstStrand":
			strand = "--rf"
		elif first_line == "SecondStrand":
			strand = "--fr"
		else:
			strand = ""	
		shell(PATH+"/executables/"+platform_info()+"/stringtie {input.bam} -G {input.annotation} {strand} -o {output.gtf} -p {threads} -A {output.exp} {params.basic} > {log}")

rule stringtie_merge:
	input:
		stringtie=expand(RESULTS_PATH+"/stringtie/{}_{}.StringTie.gtf".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		annotation=os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH
	output:
		RESULTS_PATH+"/stringtie.merged.wildtypes.gtf"
	params:
		basic="-T 1",
		cluster='-N 1 -c 1 --mem=32G -t 160:00:00 -o logs/stringtie_merge.%A.{sample}.log'
	log:
		RESULTS_PATH+"/logs/stringtie_merge.txt"
	run:
		inputList = [ string for string in input["stringtie"] if re.search("Untreated|siLuc|WT|WildType|Senescence]", string) ]
		shell("stringtie --merge {params.basic} -G {input.annotation} -o {output} {inputList} 2>&1 > {log}")

rule create_flatGFF:
	input:
		gff=os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH,
		exe=PATH+"/executables/jar/QoRTs.jar"
	output:
		os.path.splitext(os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH)[0]+".flat.gtf"
	params:
		cluster='-N 1 -c 1 --mem=16G -t 160:00:00 -o logs/create_flatGFF.%A.log'
	shell:
		"java -Xmx12G -jar {input.exe} makeFlatGff --stranded --quiet {input.gff} {output}"

rule quantification_SE:
	input:
		exe=PATH+"/executables/jar/QoRTs.jar",
		gff=os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH,
		flatfile=os.path.splitext(os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH)[0]+".flat.gtf",
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["SE"]),
		index=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam.bai", ext=["SE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["SE"]),
		chrmlen=STAR_INDEX_PATH+"/chrNameLength.txt"
	output:
		expand(RESULTS_PATH+"/qorts/{{sample}}_{ext}/QC.geneCounts.detailed.txt.gz", ext=["SE"]),
		expand(RESULTS_PATH+"/qorts/{{sample}}_{ext}/QC.geneCounts.formatted.for.DESeq.txt.gz", ext=["SE"])
	params:
		library="--singleEnded --minMAPQ 255",
		outdir=RESULTS_PATH+"/qorts/{sample}_SE",
		functions="StrandCheck,GeneCalcs,writeKnownSplices,writeNovelSplices,chromCounts,writeGenewiseGeneBody,writeBiotypeCounts,calcDetailedGeneCounts,writeDESeq,writeDEXSeq,writeJunctionSeqCounts",
		cluster='-N 1 -c 1 --mem=16G -t 160:00:00 -o logs/quantification.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/qorts/{{sample}}_{ext}.txt", ext=["SE"])
	run:
		with open(input["std"][0], "r") as infile:
			first_line = infile.readline()
		if first_line == "SecondStrand":
			shell(" ".join(("java -Xmx12G -jar", input["exe"], "QC --stranded", params["library"], "--runFunctions", params["functions"], "--chromSizes", input["chrmlen"], input["bamfile"][0], input["gff"], params["outdir"])))
		elif first_line == "FirstStrand":
			shell(" ".join(("java -Xmx12G -jar", input["exe"], "QC --stranded --stranded_fr_secondstrand", params["library"], "--runFunctions", params["functions"], "--chromSizes", input["chrmlen"], input["bamfile"][0], input["gff"], params["outdir"])))
		else:
			shell(" ".join(("java -Xmx12G -jar", input["exe"], "QC", params["library"], "--runFunctions", params["functions"], "--chromSizes", input["chrmlen"], input["bamfile"][0], input["gff"], params["outdir"])))

rule quantification_PE:
	input:
		exe=PATH+"/executables/jar/QoRTs.jar",
		picard=PATH+"/executables/jar/picard.jar",
		gff=os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH,
		flatfile=os.path.splitext(os.path.splitext(ANNOTATION_PATH)[0] if os.path.splitext(ANNOTATION_PATH)[1] == ".gz" else ANNOTATION_PATH)[0]+".flat.gtf",
		bamfile=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam", ext=["PE"]),
		index=expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.sortedByCoord.bam.bai", ext=["PE"]),
		std=expand(RESULTS_PATH+"/mapped/{{sample}}_{ext}.StrandCheck.out.tab", ext=["PE"]),
		chrmlen=STAR_INDEX_PATH+"/chrNameLength.txt"
	output:
		fixed=temp(expand(RESULTS_PATH+"/picarded/{{sample}}_{ext}.Aligned.FixedMate.bam", ext=["PE"])),
		detail=expand(RESULTS_PATH+"/qorts/{{sample}}_{ext}/QC.geneCounts.detailed.txt.gz", ext=["PE"]),
		deseq=expand(RESULTS_PATH+"/qorts/{{sample}}_{ext}/QC.geneCounts.formatted.for.DESeq.txt.gz", ext=["PE"])
	params:
		library="--minMAPQ 255",
		outdir=RESULTS_PATH+"/qorts/{sample}_PE",
		functions="StrandCheck,GeneCalcs,writeKnownSplices,writeNovelSplices,chromCounts,writeGenewiseGeneBody,writeBiotypeCounts,calcDetailedGeneCounts,writeDESeq,writeDEXSeq,writeJunctionSeqCounts",
		cluster='-N 1 -c 1 --mem=16G -t 160:00:00 -o logs/quantification.%A.{sample}.log'
	log:
		expand(RESULTS_PATH+"/logs/qorts/{{sample}}_{ext}.txt", ext=["SE"])
	run:
		shell("".join(("java -Xmx12G -jar ", input["picard"], " FixMateInformation I=", input["bamfile"][0], " O=", output["fixed"][0])))
		with open(input["std"][0], "r") as infile:
			first_line = infile.readline()
		if first_line == "SecondStrand":
			shell(" ".join(("java -Xmx12G -jar", input["exe"], "QC --stranded", params["library"], "--runFunctions", params["functions"], "--chromSizes", input["chrmlen"], input["bamfile"][0], input["gff"], params["outdir"])))
		elif first_line == "FirstStrand":
			shell(" ".join(("java -Xmx12G -jar", input["exe"], "QC --stranded --stranded_fr_secondstrand", params["library"], "--runFunctions", params["functions"], "--chromSizes", input["chrmlen"], input["bamfile"][0], input["gff"], params["outdir"])))
		else:
			shell(" ".join(("java -Xmx12G -jar", input["exe"], "QC", params["library"], "--runFunctions", params["functions"], "--chromSizes", input["chrmlen"], input["bamfile"][0], input["gff"], params["outdir"])))

rule metrics:
	input:
		trimmed=expand(RESULTS_PATH+"/logs/trim_galore/{}_{}.fq.gz_trimming_report.txt".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		ribosomal=expand(RESULTS_PATH+"/logs/rRNA_tRNA_removed/{}_{}_rRNA_tRNA_removed.log".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		mapped=expand(RESULTS_PATH+"/mapped/{}_{}.Log.final.out".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		strand=expand(RESULTS_PATH+"/mapped/{}_{}.StrandCheck.out.tab".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		duplicated=expand(RESULTS_PATH+"/picarded/{}_{}.Aligned.sortedByCoord.txt".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		qorts=expand(RESULTS_PATH+"/qorts/{}_{}/QC.summary.txt".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
	output:
		RESULTS_PATH+"/sampleMetrics.tsv"
	params:
		cluster='-N 1 -c 1 --mem=4GB -t 160:00:00 -o logs/report.%A.log'
	run:
		idsList = [ string for string in config["samples"] ]
		iniList = [ string for string in shell("for infile in {input.trimmed}; do grep -m 1 'sequences processed in total' $infile | awk '{{print $1}}'; done", iterable=True) ]
		triList = [ string for string in shell("for infile in {input.trimmed}; do grep -m 1 'shorter than the length cutoff' $infile | awk '{{print $(NF-1)}}'; done", iterable=True) ]
		ribList = [ string for string in shell("for infile in {input.ribosomal}; do grep '1 time' $infile | awk '{{s+=$1}}END{{print s}}'; done", iterable=True) ]
		uniList = [ string for string in shell("for infile in {input.mapped}; do grep 'Uniquely mapped reads number' $infile | awk '{{print $NF}}'; done", iterable=True) ]
		mulList = [ string for string in shell("for infile in {input.mapped}; do grep 'Number of reads mapped to too many loci' $infile | awk '{{print $NF}}'; done", iterable=True) ]
		strList = [ string for string in shell("for infile in {input.strand}; do awk '{{print $1}}' $infile; done", iterable=True) ]
		valList = [ string for string in shell("for infile in {input.duplicated}; do grep -A 1 $infile | tail -1 | awk -F '\\t' '{{print $2+$3}}'; done", iterable=True) ]
		dupList = [ string for string in shell("for infile in {input.duplicated}; do grep -A 1 $infile | tail -1 | awk -F '\\t' '{{print $6+$7}}'; done", iterable=True) ]
		finList = [ string for string in shell("for infile in {input.duplicated}; do grep -A 1 $infile | tail -1 | awk -F '\\t' '{{print $NF}}'; done", iterable=True) ]
		exoList = [ string for string in shell("for infile in {input.qorts}; do grep -w 'ReadPairs_UniqueGene' $infile | awk '{{print $2}}'; done", iterable=True) ]
		intList = [ string for string in shell("for infile in {input.qorts}; do grep -w 'ReadPairs_NoGene_Intron' $infile | awk '{{print $2}}'; done", iterable=True) ]
		splList = [ string for string in shell("for infile in {input.qorts}; do grep -w 'SpliceEvents' $infile | awk '{{print $2}}'; done", iterable=True) ]
		ambList = [ string for string in shell("for infile in {input.qorts}; do grep -w 'ReadPairs_AmbigGene' $infile | awk '{{print $2}}'; done", iterable=True) ]
		nogList = [ string for string in shell("for infile in {input.qorts}; do grep 'ReadPairs_NoGene_' $infile | grep -v 'Intron' | awk '{{s+=$2}}END{{print s}}'; done", iterable=True) ]
		DF = pd.DataFrame({"Total reads" : iniList, "Trimmed": triList, "Ribosomal" : ribList, "Uniquely mapped": uniList, "Multi-hits mapped" : mulList,
						   "Library strand" : strList, "Valid": valList, "Duplicated" : dupList, "Usable reads": finList,
						   "Exonic" : exoList, "Intronic": intList, "Spliced" : splList, "Ambiguous": ambList, "Intergenic" : nogList,}, index=idsList)
		DF.to_csv(output[0], sep="\t", index_label="Sample")

rule report:
	input:
		bam=expand(RESULTS_PATH+"/picarded/{}_{}.Aligned.sortedByCoord.bam.bai".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		strand=expand(RESULTS_PATH+"/mapped/{}_{}.StrandCheck.out.tab".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		bigwig=expand(RESULTS_PATH+"/coverage/{}_{}.bigwig".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		cpmbw=expand(RESULTS_PATH+"/coverage/{}_{}_{}_CPM.bigwig".format(sample, ext, typ) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] ) for typ in ["plus", "minus", "unstranded"]),
		qorts=expand(RESULTS_PATH+"/qorts/{}_{}/QC.geneCounts.detailed.txt.gz".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] ))
	output:
		RESULTS_PATH+"/sampleTable.tsv"
	params:
		cluster='-N 1 -c 1 --mem=4GB -t 160:00:00 -o logs/report.%A.log'
	run:
		idsList = [ string for string in config["samples"] ]
		bamList = [ os.path.basename(string).replace(".bai", "") for string in input["bam"] ]
		cntList = [ os.path.basename(os.path.dirname(string))+"/"+os.path.basename(string) for string in input["qorts"] ]
		stdList = [ os.path.basename(string) for string in input["strand"] ]
		wigList = [ os.path.basename(string) for string in input["bigwig"] ]
		DF = pd.DataFrame({"BamFile" : bamList, "GeneCount": cntList, "LibraryStrand" : stdList, "BigWig": wigList}, index=idsList)
		DF.to_csv(output[0], sep="\t", index_label="Sample")

rule report_long:
	input:
		bam=expand(RESULTS_PATH+"/picarded/{}_{}.Aligned.sortedByCoord.bam.bai".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		strand=expand(RESULTS_PATH+"/mapped/{}_{}.StrandCheck.out.tab".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		bigwig=expand(RESULTS_PATH+"/coverage/{}_{}.bigwig".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		cpmbw=expand(RESULTS_PATH+"/coverage/{}_{}_{}_CPM.bigwig".format(sample, ext, typ) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] ) for typ in ["plus", "minus", "unstranded"]),
		qorts=expand(RESULTS_PATH+"/qorts/{}_{}/QC.geneCounts.detailed.txt.gz".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		stringtie=expand(RESULTS_PATH+"/stringtie/{}_{}.StringTie.gtf".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] ))
	output:
		RESULTS_PATH+"/sampleTableLong.tsv"
	params:
		cluster='-N 1 -c 1 --mem=4GB -t 160:00:00 -o logs/report.%A.log'
	run:
		idsList = [ string for string in config["samples"] ]
		bamList = [ os.path.basename(string).replace(".bai", "") for string in input["bam"] ]
		cntList = [ os.path.basename(os.path.dirname(string))+"/"+os.path.basename(string) for string in input["qorts"] ]
		stdList = [ os.path.basename(string) for string in input["strand"] ]
		wigList = [ os.path.basename(string) for string in input["bigwig"] ]
		strList = [ os.path.basename(string) for string in input["stringtie"] ]
		DF = pd.DataFrame({"BamFile" : bamList, "GeneCount": cntList, "LibraryStrand" : stdList, "BigWig": wigList, "StringTie (GTF)" : strList}, index=idsList)
		DF.to_csv(output[0], sep="\t", index_label="Sample")

# Individual dag
rule dag:
	output:
		RESULTS_PATH+"/dag.pdf",
		RESULTS_PATH+"/dag.svg"
	params:
		expand(RESULTS_PATH+"/fastqc-pre/{}_{}_fastqc.html".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/picarded/{}_{}.Aligned.sortedByCoord.bam".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/fastqc-post/{}_{}_fastqc.html".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/coverage/{}_{}.bigwig".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/stringtie/{}_{}.StringTie.gtf".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/qorts/{}_{}/QC.geneCounts.detailed.txt.gz".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] ))
	run:
		shell("snakemake --dag {} | dot -Tpdf > {}".format(" ".join([ item[0] for item in params]), output[0]))
		shell("snakemake --dag {} | dot -Tsvg > {}".format(" ".join([ item[0] for item in params]), output[1]))

# Complete dag
rule dag_complete:
	output:
		RESULTS_PATH+"/dag_complete.pdf",
		RESULTS_PATH+"/dag_complete.svg"
	params:
		expand(RESULTS_PATH+"/fastqc-pre/{}_{}_fastqc.html".format(sample, ext) for sample in config["samples"] for ext in ( ["F", "R"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/fastqc-post/{}_{}_fastqc.html".format(sample, ext) for sample in config["samples"] for ext in ( ["F", "R"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/picarded/{}_{}.Aligned.sortedByCoord.bam".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/coverage/{}_{}.bigwig".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/stringtie/{}_{}.StringTie.gtf".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		expand(RESULTS_PATH+"/qorts/{}_{}/QC.geneCounts.detailed.txt.gz".format(sample, ext) for sample in config["samples"] for ext in ( ["PE"] if len(config["samples"][sample])==2 else ["SE"] )),
		[RESULTS_PATH+"/sampleTable.tsv"]
	run:
		shell("snakemake --dag {} | dot -Tpdf > {}".format(" ".join(list(chain.from_iterable(params))), output[0]))
		shell("snakemake --dag {} | dot -Tsvg > {}".format(" ".join(list(chain.from_iterable(params))), output[1]))
